{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Analytics- EN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrtCrTRxgcWQ"
      },
      "source": [
        "# **A. Sentimen Analysis - Bahasa Inggris**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q88XvREJgp37"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCZOfN6ogzaF"
      },
      "source": [
        "* VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
        "Source : https://github.com/cjhutto/vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaFaCfvTgymM"
      },
      "source": [
        "# Install Library\n",
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q70xSuPxg4s2"
      },
      "source": [
        "* Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
        "NLTK is a leading platform for building Python programs to work with human language data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBUIZ_BehBJg"
      },
      "source": [
        "# Import Library\n",
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2HCzmpDhMGp"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WwKN7XBhKDh"
      },
      "source": [
        "# Import Data from Github\n",
        "url = 'https://raw.githubusercontent.com/amaliaristantya/Text-Analytics/main/General%20Motor.csv'\n",
        "df = pd.read_csv(url, sep=',',)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AzNNGU1hV-H"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20gYr6rLhaPm"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwmsMNhjj_je"
      },
      "source": [
        "## Pemodelan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_knPRiGvkJQP"
      },
      "source": [
        "#Change Title to String\n",
        "df['text'] = df['text'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbg54wuNkTCo"
      },
      "source": [
        "# Import library for Text Analytics\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWI5RvMCkVWL"
      },
      "source": [
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "listy = [] \n",
        "for index, row in df.iterrows():\n",
        "  df['text']\n",
        "  ss = sid.polarity_scores(row['text'])\n",
        "  listy.append(ss)\n",
        "  \n",
        "se = pd.Series(listy)\n",
        "df['polarity'] = se.values\n",
        "display(df.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXLHr_wrkZTY"
      },
      "source": [
        "# Pie Chart\n",
        "import matplotlib.pyplot as plt\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qognmYcckZ6W"
      },
      "source": [
        "df.to_csv('Output_File.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Bgypxbk2-4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **B. Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neygvX-tmRtx"
      },
      "source": [
        "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP3aURQ7maVB"
      },
      "source": [
        "## Install & Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBon9OvSmYpj"
      },
      "source": [
        "# Install pyLDAvis\n",
        "! pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zQ924oRmwIQ"
      },
      "source": [
        "# Import Libraries\n",
        "import nltk\n",
        "import os\n",
        "import numpy as np, pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
        "\n",
        "# Import Modules\n",
        "from __future__ import print_function \n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WASdXMZPm__P"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9GLdRA6nMJA"
      },
      "source": [
        "# Clone Data from Github\n",
        "! git clone https://github.com/amaliaristantya/Text-Analytics\n",
        "\n",
        "# Set Data Directory\n",
        "os.chdir('Text-Analytics')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYyZZrLynMzc"
      },
      "source": [
        "# Import Data\n",
        "nltk.download('stopwords')\n",
        "data_file = 'General Motor.csv'\n",
        "n_topics, Top_Topics, Top_Words = 4, 5, 5 # Depends on the purpose of analytics\n",
        "max_df, min_df = 0.75, 10 # Can be adjusted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmN6Zv46nP8C"
      },
      "source": [
        "# Import Library\n",
        "import MyLib as TS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJg6eZGunZ-_"
      },
      "source": [
        "## Pemodelan "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cKvd3RlnYmt"
      },
      "source": [
        "# Import Stop Words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Import Data\n",
        "data_file = 'General Motor.csv'\n",
        "\n",
        "# Load Tweets Data\n",
        "import MyLib as TS\n",
        "Tweets = TS.LoadTxt(data_file) \n",
        "print('Total loaded tweets = {0}'.format(len(Tweets)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f_cGP-5nmDR"
      },
      "source": [
        "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',stop_words = 'english', lowercase = True, token_pattern = r'\\b[a-zA-Z]{3,}\\b',max_df = max_df, min_df = min_df)\n",
        "dtm_tf = tf_vectorizer.fit_transform(Tweets)\n",
        "tf_terms = tf_vectorizer.get_feature_names()\n",
        "del Tweets\n",
        "print('Done Calculating VSM ... ', flush = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTEICpkGnpar"
      },
      "source": [
        "# LDA Topics\n",
        "lda_tf = LatentDirichletAllocation(n_components=n_topics, learning_method='online', random_state=0).fit(dtm_tf)\n",
        "print('Done LDA topics ... ', flush = True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tgO-FiwnuEI"
      },
      "source": [
        "vsm_topics = lda_tf.transform(dtm_tf); doc_topic =  [a.argmax()+1 for a in tqdm(vsm_topics)] # topic of docs\n",
        "print('In total there are {0} major topics, distributed as follows'.format(len(set(doc_topic))))\n",
        "plt.hist(np.array(doc_topic), alpha=0.5); plt.show()\n",
        "print('Printing top {0} Topics, with top {1} Words:'.format(Top_Topics, Top_Words))\n",
        "TS.print_Topics(lda_tf, tf_terms, Top_Topics, Top_Words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj_h8GCGnxrP"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer) # Interactively visualizing the Topics, please ignore the Warnings\n",
        "# Wait few minutes and then hover the Mouse over the Topics to Explore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8foJgAbMcFH"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **C. Text Network**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0EHjS_gXozP"
      },
      "source": [
        "Based on Social Network Analysis (SNA), implement on wod/phases/ n-gram that act as actor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG9ZnECmvQ9r"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2AUtl6XMmus"
      },
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import community\n",
        "import seaborn as sns\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0bjyibhvXm9"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGpcrETDMrXU"
      },
      "source": [
        "# Import Data from Github\n",
        "url = 'https://raw.githubusercontent.com/amaliaristantya/Text-Analytics/main/General%20Motor-TNA.csv'\n",
        "df_tna = pd.read_csv(url, sep=';')\n",
        "df_tna.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W-wOye4vd9T"
      },
      "source": [
        "## Pemodelan Jaringan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtRyuN9nNpcL"
      },
      "source": [
        "# Contstruct a Network\n",
        "G1 = nx.from_pandas_edgelist(df_tna)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I7FO5PENhwW"
      },
      "source": [
        "# Degree Centrality\n",
        "degree = nx.degree_centrality(G1)\n",
        "\n",
        "# Sorted from the Highest\n",
        "sorted(nx.degree(G1), key=lambda x: x[1], reverse=True)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMm2Gg6SM1N1"
      },
      "source": [
        "# Set Degree Dictionary\n",
        "d = dict(degree)\n",
        "\n",
        "# Contstruct a Network\n",
        "G1 = nx.from_pandas_edgelist(df_tna)\n",
        "\n",
        "# Visualize the Network\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(50,50))\n",
        "nx.draw(G1, with_labels=True, \n",
        "        node_color='skyblue', nodelist=d.keys(),\n",
        "        node_size=[v * 60000 for v in d.values()], \n",
        "        arrowstyle='->',arrowsize=20, edge_color='r',\n",
        "        font_size=10,\n",
        "        pos=nx.kamada_kawai_layout(G1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ID1_iUsM5dU"
      },
      "source": [
        "# Show Number of Nodes\n",
        "nx.number_of_nodes(G1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emRYzC6pM9KB"
      },
      "source": [
        "# Show Number of Edges\n",
        "nx.number_of_edges(G1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vdUYVRVNApV"
      },
      "source": [
        "# Show Graph Density\n",
        "nx.density(G1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2zdKT_WNDQ8"
      },
      "source": [
        "# Show Number of Connected Component\n",
        "nx.number_connected_components(G1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxVpH-QMNHqE"
      },
      "source": [
        "# Import Module\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "\n",
        "# Modularity Community Detection\n",
        "communities_m = sorted(greedy_modularity_communities(G1), key=len, reverse=True)\n",
        "communities_m"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}